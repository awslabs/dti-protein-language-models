{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3329b2f1",
   "metadata": {},
   "source": [
    "# DTI Network: Drug-Target Interaction with DGL Life Science and Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39982220",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!conda install -y pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=10.2 -c pytorch\n",
    "!conda install -y -c dglteam dgl-cuda10.2\n",
    "!conda install -y -c conda-forge rdkit==2018.09.3\n",
    "!conda install -y -c dglteam dgllife\n",
    "!pip install pytdc\n",
    "!pip install transformers\n",
    "!pip install d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512d29b4",
   "metadata": {},
   "source": [
    "## Generic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57e8cb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/boto3/compat.py:88: PythonDeprecationWarning: Boto3 will no longer support Python 3.6 starting May 30, 2022. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.7 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import glob\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "\n",
    "# Plotting\n",
    "sns.set_theme(style='ticks')\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# S3 Client\n",
    "import boto3\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# SageMaker\n",
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Multiprocessing\n",
    "import multiprocessing\n",
    "\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd8ae39",
   "metadata": {},
   "source": [
    "## Load benchmark data with PyTDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0dea39c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n",
      "To log space...\n"
     ]
    }
   ],
   "source": [
    "from tdc.multi_pred import DTI\n",
    "\n",
    "data = DTI(name = 'DAVIS')\n",
    "data.convert_to_log(form = 'binding')\n",
    "\n",
    "# data = DTI(name = 'KIBA')\n",
    "\n",
    "# data = DTI(name = 'BindingDB_IC50')\n",
    "# data.harmonize_affinities(mode = 'max_affinity')\n",
    "# data.convert_to_log(form = 'binding')\n",
    "\n",
    "split = data.get_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f126281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = split[\"train\"]\n",
    "valid_data = split[\"valid\"]\n",
    "test_data = split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a29b6c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    18041.000000\n",
       "mean       747.923785\n",
       "std        376.172691\n",
       "min        244.000000\n",
       "25%        478.000000\n",
       "50%        635.000000\n",
       "75%        961.000000\n",
       "max       2549.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prot_lens = pd.Series([len(x) for x in train_data.Target.tolist()])\n",
    "prot_lens.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "486ad210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 251,
       "width": 400
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data.Y.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d633edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer, MinMaxScaler\n",
    "\n",
    "target_scaler = MinMaxScaler()\n",
    "# target_scaler = FunctionTransformer(lambda x: x)\n",
    "train_data.Y = target_scaler.fit_transform(train_data.Y.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d75a8e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 251,
       "width": 393
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data.Y.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e58fd31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82360, 11766, 23531)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(valid_data), len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e33f13d",
   "metadata": {},
   "source": [
    "## DTI Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162fece6",
   "metadata": {},
   "source": [
    "Goal: better version of https://www.biorxiv.org/content/10.1101/684662v8.full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0208085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "from dgllife.utils import AttentiveFPAtomFeaturizer, AttentiveFPBondFeaturizer, smiles_to_bigraph\n",
    "from dgllife.utils import PretrainAtomFeaturizer, PretrainBondFeaturizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cc16b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTIDataset(Dataset):\n",
    "    def __init__(self, drugs, targets, affinities):\n",
    "        self.drugs = drugs\n",
    "        self.targets = targets\n",
    "        self.affinities = affinities\n",
    "        self.atom_featurizer = AttentiveFPAtomFeaturizer()\n",
    "        self.bond_featurizer = AttentiveFPBondFeaturizer()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.affinities)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        smiles = self.drugs[idx]\n",
    "        mol_graph = smiles_to_bigraph(smiles, \n",
    "                                      node_featurizer=self.atom_featurizer, \n",
    "                                      edge_featurizer=self.bond_featurizer,\n",
    "                                      )\n",
    "        sequence = self.targets[idx]\n",
    "        temp = [l for l in sequence]\n",
    "        temp = \" \".join(temp)\n",
    "        temp = re.sub(r\"[UZOB]\", \"X\", temp)\n",
    "        label = self.affinities[idx]\n",
    "        return mol_graph, temp, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7eba6bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "from torch.utils.data._utils.collate import default_collate\n",
    "\n",
    "def collate_fn(batch):\n",
    "    mol_graphs, protein_sequences, labels = tuple(zip(*batch))\n",
    "    return dgl.batch(mol_graphs), default_collate(protein_sequences), default_collate(labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b01c4e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader from PyTDC \n",
    "train_drugs = train_data.Drug.tolist()\n",
    "train_targets = train_data.Target.tolist()\n",
    "train_affinities = train_data.Y.tolist()\n",
    "train_dataset = DTIDataset(train_drugs, train_targets, train_affinities)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96ea59dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader from PyTDC \n",
    "valid_drugs = valid_data.Drug.tolist()\n",
    "valid_targets = valid_data.Target.tolist()\n",
    "valid_affinities = valid_data.Y.tolist()\n",
    "valid_dataset = DTIDataset(valid_drugs, valid_targets, valid_affinities)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5bcf403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract node and edge dimensionality from molecular graphs\n",
    "mol_graphs, protein_sequences, labels = next(iter(train_dataloader))\n",
    "mol_node_dim = mol_graphs.ndata['h'].shape[1]\n",
    "mol_edge_dim = mol_graphs.edata['e'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "899f270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgllife.model import AttentiveFPGNN, AttentiveFPReadout\n",
    "from d2l import torch as d2l\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Adapted from Mufei's DGL Life Science example\n",
    "class AttentiveFPEmbedding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 node_feat_size,\n",
    "                 edge_feat_size,\n",
    "                 num_layers=2,\n",
    "                 num_timesteps=2,\n",
    "                 graph_feat_size=200,\n",
    "                 dropout=0.):\n",
    "        super(AttentiveFPEmbedding, self).__init__()\n",
    "\n",
    "        self.gnn = AttentiveFPGNN(node_feat_size=node_feat_size,\n",
    "                                  edge_feat_size=edge_feat_size,\n",
    "                                  num_layers=num_layers,\n",
    "                                  graph_feat_size=graph_feat_size,\n",
    "                                  dropout=dropout)\n",
    "#         self.readout = AttentiveFPReadout(feat_size=graph_feat_size,\n",
    "#                                           num_timesteps=num_timesteps,\n",
    "#                                           dropout=dropout)\n",
    "\n",
    "    def forward(self, g, node_feats, edge_feats, get_node_weight=False):\n",
    "        node_feats = self.gnn(g, node_feats, edge_feats)\n",
    "        batch_num_nodes = g.batch_num_nodes().tolist()\n",
    "        return pad_sequence(torch.split(node_feats, batch_num_nodes, dim=0), batch_first=True)\n",
    "#         if get_node_weight:\n",
    "#             g_feats, node_weights = self.readout(g, node_feats, get_node_weight)\n",
    "#             return g_feats, node_weights\n",
    "#         else:\n",
    "#             g_feats = self.readout(g, node_feats, get_node_weight)\n",
    "#             return g_feats\n",
    "\n",
    "    \n",
    "class DTINetwork(nn.Module):\n",
    "    def __init__(self,\n",
    "                 prot_model,\n",
    "                 mol_node_dim=39, \n",
    "                 mol_edge_dim=10, \n",
    "                 num_layers=2,\n",
    "                 num_timesteps=2,\n",
    "                 graph_feat_size=200,\n",
    "                 hidden_dim=256, \n",
    "                 dropout=0.1,\n",
    "                 use_residue_attention=True,\n",
    "                 num_heads=4,\n",
    "                 verbose=False):\n",
    "        super(DTINetwork, self).__init__()\n",
    "        self.verbose = verbose\n",
    "        self.prot_model = prot_model\n",
    "        prot_dim = prot_model.pooler.dense.out_features\n",
    "        self.mol_model = AttentiveFPEmbedding(node_feat_size=mol_node_dim, \n",
    "                                              edge_feat_size=mol_edge_dim,\n",
    "                                              graph_feat_size=graph_feat_size,\n",
    "                                              num_layers=num_layers,\n",
    "                                              num_timesteps=num_timesteps,\n",
    "                                              dropout=dropout)\n",
    "        self.use_residue_attention = use_residue_attention\n",
    "        if self.use_residue_attention:\n",
    "            self.residue_attention = d2l.MultiHeadAttention(\n",
    "                                                         query_size=graph_feat_size,\n",
    "                                                         # query_size=mol_node_dim,\n",
    "                                                         key_size=prot_dim, \n",
    "                                                         value_size=prot_dim, \n",
    "                                                         num_hiddens=hidden_dim,\n",
    "                                                         num_heads=num_heads,\n",
    "                                                         dropout=dropout)\n",
    "            self.linear1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        else:\n",
    "            self.linear1 = nn.Linear(prot_dim+graph_feat_size, hidden_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, mol_graphs, atom_mask, encoded_proteins, protein_lens):\n",
    "        x_mol = self.mol_model(mol_graphs, \n",
    "                               node_feats=mol_graphs.ndata['h'], \n",
    "                               edge_feats=mol_graphs.edata['e'])\n",
    "        if self.verbose: print(\"Molecule tensor shape:\", x_mol.shape)\n",
    "        if self.use_residue_attention:\n",
    "            x_prot = self.prot_model(**encoded_proteins).last_hidden_state[:, 1:-1, :] # Don't use <CLS> and <SEP> tokens\n",
    "            if self.verbose: print(\"Protein tensor shape:\", x_prot.shape)\n",
    "            # x_mol = x_mol.unsqueeze(1)\n",
    "            x = self.residue_attention(queries=x_mol,\n",
    "                                       keys=x_prot, \n",
    "                                       values=x_prot,\n",
    "                                       valid_lens=atom_mask)\n",
    "            if self.verbose: print(\"Attention output shape:\", x.shape)\n",
    "            # x = x.squeeze(1)\n",
    "        else:\n",
    "            x_prot = self.prot_model(**encoded_proteins).last_hidden_state[:, 0, :]\n",
    "            x = torch.cat((x_prot, x_mol), axis=1)\n",
    "        x = self.dropout(self.activation(self.linear1(x)))\n",
    "        x = self.linear2(x)\n",
    "        return torch.sum(torch.mul(atom_mask, x.squeeze(2)), dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "561b80a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "prot_tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "prot_model = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
    "dti_model = DTINetwork(prot_model, \n",
    "                       mol_node_dim=mol_node_dim, \n",
    "                       mol_edge_dim=mol_edge_dim, \n",
    "                       use_residue_attention=True,\n",
    "                       num_heads=1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3277e23e",
   "metadata": {},
   "source": [
    "### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3476a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1]), torch.Size([2, 1]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mol_graphs, protein_sequences, labels = next(iter(valid_dataloader))\n",
    "y_true = labels.unsqueeze(1).to(device)\n",
    "\n",
    "encoded_proteins = prot_tokenizer(protein_sequences, \n",
    "                      return_tensors='pt', \n",
    "                      max_length=1024, \n",
    "                      truncation=True, \n",
    "                      padding=True, \n",
    "                      return_length=True)\n",
    "\n",
    "mol_graphs = mol_graphs.to(device)\n",
    "atoms_per_mol = mol_graphs.batch_num_nodes().tolist()\n",
    "# print(atoms_per_mol)\n",
    "atom_mask = [torch.ones(x) for x in atoms_per_mol]\n",
    "atom_mask = pad_sequence(atom_mask, batch_first=True).to(device)\n",
    "encoded_proteins = encoded_proteins.to(device)\n",
    "protein_lens = encoded_proteins.pop(key=\"length\")\n",
    "y_pred = dti_model(mol_graphs, atom_mask, encoded_proteins, protein_lens).to(torch.float64)\n",
    "y_pred.shape, y_true.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0660b4",
   "metadata": {},
   "source": [
    "### Full training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d10f73b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, train_loader, val_loader, epochs=20, device=\"cpu\"):\n",
    "    for epoch in range(1, epochs+1):\n",
    "        training_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        model.train()\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            mol_graphs, protein_sequences, labels = batch\n",
    "            y_true = labels.unsqueeze(1).to(device)\n",
    "\n",
    "            encoded_proteins = prot_tokenizer(protein_sequences, \n",
    "                                  return_tensors='pt', \n",
    "                                  max_length=1024, \n",
    "                                  truncation=True, \n",
    "                                  padding=True, \n",
    "                                  return_length=True)\n",
    "\n",
    "            mol_graphs = mol_graphs.to(device)\n",
    "            atoms_per_mol = mol_graphs.batch_num_nodes().tolist()\n",
    "            # print(atoms_per_mol)\n",
    "            atom_mask = [torch.ones(x) for x in atoms_per_mol]\n",
    "            atom_mask = pad_sequence(atom_mask, batch_first=True).to(device)\n",
    "            encoded_proteins = encoded_proteins.to(device)\n",
    "            protein_lens = encoded_proteins.pop(key=\"length\")\n",
    "            y_pred = dti_model(mol_graphs, atom_mask, encoded_proteins, protein_lens).to(torch.float64)\n",
    "            loss = loss_fn(y_pred, y_true)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_loss = loss.data.item()\n",
    "            training_loss += batch_loss\n",
    "            if (i+1) % 100 == 0:\n",
    "                print('Batch: {}, Training Loss: {:.2f}'.format(i+1, training_loss / (i+1)))\n",
    "        training_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        for batch in val_loader:\n",
    "            mol_graphs, protein_sequences, labels = batch\n",
    "            y_true = labels.unsqueeze(1).to(device)\n",
    "\n",
    "            encoded_proteins = prot_tokenizer(protein_sequences, \n",
    "                                  return_tensors='pt', \n",
    "                                  max_length=1024, \n",
    "                                  truncation=True, \n",
    "                                  padding=True, \n",
    "                                  return_length=True)\n",
    "\n",
    "            mol_graphs = mol_graphs.to(device)\n",
    "            atoms_per_mol = mol_graphs.batch_num_nodes().tolist()\n",
    "            # print(atoms_per_mol)\n",
    "            atom_mask = [torch.ones(x) for x in atoms_per_mol]\n",
    "            atom_mask = pad_sequence(atom_mask, batch_first=True).to(device)\n",
    "            encoded_proteins = encoded_proteins.to(device)\n",
    "            protein_lens = encoded_proteins.pop(key=\"length\")\n",
    "            y_pred = dti_model(mol_graphs, atom_mask, encoded_proteins, protein_lens).to(torch.float64)\n",
    "            loss = loss_fn(y_pred, y_true) \n",
    "            valid_loss += loss.data.item()\n",
    "        valid_loss /= len(val_loader.dataset)\n",
    "\n",
    "        print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}'.format(epoch, training_loss, valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "872d45b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 15.78 GiB total capacity; 13.56 GiB already allocated; 21.75 MiB free; 14.14 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-c100ce91d1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0mval_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m       device=device)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-f6249f168883>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, loss_fn, train_loader, val_loader, epochs, device)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mencoded_proteins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded_proteins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mprotein_lens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded_proteins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdti_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmol_graphs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matom_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_proteins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotein_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-62e5e9003830>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, mol_graphs, atom_mask, encoded_proteins, protein_lens)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Molecule tensor shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_mol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_residue_attention\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mx_prot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mencoded_proteins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Don't use <CLS> and <SEP> tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Protein tensor shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_prot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;31m# x_mol = x_mol.unsqueeze(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m         )\n\u001b[1;32m   1008\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    590\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m                 )\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         layer_output = apply_chunking_to_forward(\n\u001b[0;32m--> 514\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m         )\n\u001b[1;32m    516\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   2926\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2928\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 15.78 GiB total capacity; 13.56 GiB already allocated; 21.75 MiB free; 14.14 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(dti_model.parameters())\n",
    "epochs = 5\n",
    "\n",
    "train(model=dti_model, \n",
    "      loss_fn=criterion, \n",
    "      optimizer=optimizer,\n",
    "      train_loader=train_dataloader,\n",
    "      val_loader=valid_dataloader,\n",
    "      epochs=epochs,\n",
    "      device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e497729a",
   "metadata": {},
   "source": [
    "## Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452fddcb",
   "metadata": {},
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a4d47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdc import BenchmarkGroup\n",
    "group = BenchmarkGroup(name = 'DTI_DG_Group', path = 'data/')\n",
    "benchmark = group.get('BindingDB_Patent')\n",
    "\n",
    "predictions = {}\n",
    "name = benchmark['name']\n",
    "train_val, test = benchmark['train_val'], benchmark['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53fe0e8",
   "metadata": {},
   "source": [
    "### BindingDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cb04a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdc.multi_pred import DTI\n",
    "data = DTI(name = 'BindingDB_Kd')\n",
    "data.harmonize_affinities(mode = 'mean')\n",
    "split = data.get_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f464551",
   "metadata": {},
   "outputs": [],
   "source": [
    "split[\"train\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc5f810",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(split[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2771fefa",
   "metadata": {},
   "source": [
    "### DAVIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b740df38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdc.multi_pred import DTI\n",
    "\n",
    "data = DTI(name = 'DAVIS')\n",
    "split = data.get_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2452299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "split[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7082617d",
   "metadata": {},
   "source": [
    "### KIBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c44c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdc.multi_pred import DTI\n",
    "\n",
    "data = DTI(name = 'KIBA')\n",
    "split = data.get_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be152c9",
   "metadata": {},
   "source": [
    "## PyTorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348e5585",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
